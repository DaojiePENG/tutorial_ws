{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc882661",
   "metadata": {},
   "source": [
    "# Lesson10：Showcase! - 成果展與未來展望"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c3429c",
   "metadata": {},
   "source": [
    "教學主題：分享與慶祝。\n",
    "\n",
    "核心目標：展示成果，建立自信。了解下一步可以學什麼，保持學習熱情。\n",
    "\n",
    "前 60 分鐘 (機器人嘉年華)：\n",
    "每組上台展示專案，並互相體驗同學的作品。\n",
    "\n",
    "後 60 分鐘 (結業與展望)：\n",
    "Q&A，介紹 ROS、強化學習，頒發證書與合影。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c941addf",
   "metadata": {},
   "source": [
    "## 10.1 机器嘉年华-展示你的作品\n",
    "\n",
    "参考下面例子将之前学习的机器狗和机器人的接口初始化出来进行使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec46911",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "from unitree_sdk2py.core.channel import ChannelSubscriber, ChannelFactoryInitialize\n",
    "from unitree_sdk2py.go2.sport.sport_client import SportClient\n",
    "from unitree_sdk2py.g1.loco.g1_loco_client import LocoClient\n",
    "\n",
    "ChannelFactoryInitialize(0, \"ens37\")\n",
    "# Initialize GO2 Sport Client and G1 Loco Client\n",
    "go2_sport_client = SportClient()  \n",
    "go2_sport_client.SetTimeout(10.0)\n",
    "go2_sport_client.Init()\n",
    "\n",
    "# Initialize G1 Loco Client\n",
    "g1_sport_client = LocoClient()  \n",
    "g1_sport_client.SetTimeout(10.0)\n",
    "g1_sport_client.Init()\n",
    "\n",
    "# Initialize G1 Arm Action Client\n",
    "from unitree_sdk2py.g1.arm.g1_arm_action_client import G1ArmActionClient\n",
    "armAction_client = G1ArmActionClient()  \n",
    "armAction_client.SetTimeout(10.0)\n",
    "armAction_client.Init()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff15708d",
   "metadata": {},
   "source": [
    "## 10.2 ROS介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcd8c14",
   "metadata": {},
   "source": [
    "机器人操作系统（Robot Operating System，简称 ROS）并不是一个传统意义上的“操作系统”（如 Windows 或 Linux），而是一个专为机器人开发设计的灵活框架和中间件集合。它提供了一系列工具、库、协议和约定，旨在简化复杂、异构机器人系统的软件开发。\n",
    "\n",
    "### 一、核心定位与目标\n",
    "\n",
    "目的：让机器人开发者能复用代码、快速原型开发、跨平台协作。\n",
    "\n",
    "口号：“Don’t reinvent the wheel.”（不要重复造轮子）\n",
    "\n",
    "哲学：模块化、松耦合、基于通信的架构。\n",
    "\n",
    "### 二、ROS 的核心特性\n",
    "1. 分布式节点架构（Node-based）\n",
    "\n",
    "* 机器人功能被拆分为多个独立进程，称为 节点（Nodes）。\n",
    "* 节点之间通过 话题（Topics）、服务（Services）、动作（Actions） 等机制通信。\n",
    "* 支持跨机器运行（例如：传感器在一台电脑，控制算法在另一台）。\n",
    "\n",
    "2. 通信机制\n",
    "\n",
    "| 机制 | 特点 | 适用场景 |\n",
    "|------|------|--------|\n",
    "| Topic（话题） | 发布/订阅模式，异步、单向 | 传感器数据流（如激光雷达、摄像头） |\n",
    "| Service（服务） | 请求/响应模式，同步、双向 | 一次性任务（如获取地图、保存配置） |\n",
    "| Action（动作） | 带反馈的长期任务 | 需要进度反馈的任务（如导航到某点） |\n",
    "\n",
    "3. 丰富的工具链\n",
    "\n",
    "* rviz：3D 可视化工具，可显示点云、地图、机器人模型等。\n",
    "* rqt：插件式 GUI 工具集（如绘图、日志查看、参数调整）。\n",
    "* rosbag：记录和回放传感器数据，用于离线测试。\n",
    "* gazebo / Ignition：高保真物理仿真环境（常与 ROS 集成）。\n",
    "\n",
    "4. 庞大的开源生态\n",
    "\n",
    "* 官方维护数千个功能包（如 navigation、moveit、perception）。\n",
    "* 社区贡献涵盖 SLAM、路径规划、机械臂控制、计算机视觉等。\n",
    "* 支持多种传感器（Kinect、Velodyne、Realsense）、执行器和机器人平台（TurtleBot、PR2、宇树、Boston Dynamics Spot 等）。\n",
    "\n",
    "### 三、ROS 的版本演进\n",
    "\n",
    "| 版本 | 发布时间 | 状态 | 备注 |\n",
    "|------|--------|------|------|\n",
    "| ROS 1（如 Kinetic, Melodic, Noetic） | 2010 起 | 逐步淘汰 | 基于 TCPROS/UDPROS，主从架构（依赖 roscore） |\n",
    "| ROS 2（如 Foxy, Humble, Iron, Jazzy） | 2017 起 | 主流发展方向 | 基于 DDS（Data Distribution Service），去中心化、实时性更好、支持多语言（C++, Python, Rust） |\n",
    "\n",
    "✅ 当前推荐：新项目应优先使用 ROS 2（尤其是 Humble 或 Iron），因其更安全、更现代、更适合工业部署。\n",
    "### 四、ROS 2 的关键改进\n",
    "\n",
    "| 特性 | ROS 1 | ROS 2 |\n",
    "|------|-------|--------|\n",
    "| 通信中间件 | 自研 TCP/UDP | 标准 DDS（如 Fast DDS, Cyclone DDS） |\n",
    "| 实时性 | 弱 | 支持硬实时（配合 RT Linux） |\n",
    "| 安全性 | 无内置安全机制 | 支持 DDS Security（认证、加密） |\n",
    "| 生命周期管理 | 无 | 节点可管理启动/暂停/关闭状态 |\n",
    "| 跨平台 | 主要 Linux | 支持 Linux、Windows、macOS、RTOS |\n",
    "| 语言支持 | C++/Python 为主 | 官方支持 C++/Python，社区支持 Rust/Java |\n",
    "\n",
    "\n",
    "\n",
    "### 五、典型应用场景\n",
    "\n",
    "移动机器人导航：SLAM + 路径规划（如 nav2 包）\n",
    "\n",
    "机械臂控制：运动学求解、抓取规划（如 MoveIt2）\n",
    "\n",
    "多机器人协同：编队、任务分配\n",
    "\n",
    "自动驾驶原型：感知-决策-控制 pipeline\n",
    "\n",
    "教育与科研：高校机器人课程标准平台\n",
    "\n",
    "### 七、局限与挑战\n",
    "\n",
    "学习曲线陡峭：概念多（node, topic, msg, srv, launch, param...）\n",
    "\n",
    "调试复杂：分布式系统故障排查困难\n",
    "\n",
    "性能开销：序列化/反序列化、网络通信带来延迟\n",
    "\n",
    "工业部署门槛高：需容器化、安全加固、实时优化\n",
    "\n",
    "### 总结\n",
    "ROS 是机器人领域的“Linux + GitHub + Docker”三位一体：\n",
    "\n",
    "提供基础运行环境（中间件），\n",
    "\n",
    "汇聚全球开发者智慧（开源生态），\n",
    "\n",
    "支持快速构建与迭代（工具链）。\n",
    "\n",
    "尽管存在挑战，但 ROS（尤其是 ROS 2）已成为机器人软件开发的事实标准，是进入智能机器人领域的必备技能。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad2a966",
   "metadata": {},
   "source": [
    "## 10.3 强化学习介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87888e45",
   "metadata": {},
   "source": [
    "强化学习（Reinforcement Learning, RL）是一种通过与环境交互来学习最优策略的机器学习范式，特别适合解决序列决策问题。在机器人领域——尤其是高自由度、强非线性、易失稳的四足和人形机器人中，强化学习已成为突破传统控制方法局限的关键技术。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4881a2ae",
   "metadata": {},
   "source": [
    "### 一、强化学习基础原理\n",
    "核心要素（马尔可夫决策过程 MDP）：\n",
    "智能体（Agent）：机器人控制器\n",
    "环境（Environment）：物理世界或仿真器（如 Isaac Gym、PyBullet）\n",
    "状态（State）：机器人当前姿态、速度、关节角等\n",
    "动作（Action）：发送给电机的扭矩或目标位置\n",
    "奖励（Reward）：设计的目标函数（如前进速度 - 能耗 - 倾倒惩罚）\n",
    "策略（Policy）：从状态到动作的映射（通常由神经网络表示）\n",
    "目标：最大化累积折扣奖励 $max_{\\pi}E[\\sum_{t=0}^{\\infty}\\gamma^t r_t]$\n",
    "\n",
    "常用算法：\n",
    "On-policy：PPO（Proximal Policy Optimization）——稳定、易调参，广泛用于机器人\n",
    "Off-policy：SAC（Soft Actor-Critic）、TD3 ——样本效率高\n",
    "模仿学习 + RL：先用人类演示预训练（Behavior Cloning），再用 RL 微调\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbddccf5",
   "metadata": {},
   "source": [
    "### 二、为何强化学习适合腿式机器人？\n",
    "\n",
    "传统控制方法（如 LQR、MPC）依赖精确动力学模型，在复杂地形或高速运动下难以泛化。而 RL 具备以下优势：\n",
    "\n",
    "| 挑战 | RL 的应对 |\n",
    "|------|----------|\n",
    "| 高维非线性动力学 | 神经网络可拟合复杂映射 |\n",
    "| 接触不确定性（脚与地面碰撞） | 在仿真中大量采样接触事件 |\n",
    "| 多目标权衡（速度/稳定/能耗） | 通过奖励函数灵活设计 |\n",
    "| 未知地形适应 | 训练时加入随机扰动（Domain Randomization） |\n",
    "\n",
    "### 三、在四足机器人中的应用（如宇树 Go2、MIT Cheetah、ANYmal）\n",
    "典型任务：\n",
    "\n",
    "* 全地形行走\n",
    "\n",
    "    * 训练策略在斜坡、碎石、草地、楼梯上自适应步态\n",
    "示例：ETH 的 ANYmal 使用 RL 实现在野外连续行走数公里\n",
    "\n",
    "* 抗干扰与恢复\n",
    "\n",
    "    * 被推倒后自动“鲤鱼打挺”站起\n",
    "    * 遇到外力扰动时实时调整重心（如 Go2 的 AI 模式翻滚）\n",
    "\n",
    "* 高动态动作\n",
    "\n",
    "    * 小跑、跳跃、后空翻（Unitree Go2 已展示）\n",
    "    * 通过课程学习（Curriculum Learning）逐步提升难度\n",
    "\n",
    "* 节能步态优化\n",
    "    * 奖励函数包含能耗项，学习最省电的步频与相位\n",
    "\n",
    "✅ 关键技巧：\n",
    "\n",
    "使用 高度逼真的仿真器（如 NVIDIA Isaac Sim）\n",
    "域随机化（随机质量、摩擦系数、延迟）提升 sim-to-real 迁移能力\n",
    "奖励塑形（Reward Shaping）避免稀疏奖励问题（如“只在前进时给正奖励”）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22377e83",
   "metadata": {},
   "source": [
    "### 四、在人形机器人中的应用（如宇树 G1、Tesla Optimus、Boston Dynamics Atlas）\n",
    "\n",
    "人形机器人自由度更高（20–50+ DOF）、平衡更难，RL 的作用更为关键：\n",
    "\n",
    "典型突破：\n",
    "\n",
    "* 动态行走与跑步\n",
    "    * G1 实现 >2 m/s 的奔跑，依赖 PPO 在仿真中训练数百万步\n",
    "    * 传统 ZMP 控制难以实现如此高速\n",
    "\n",
    "* 复杂动作技能库\n",
    "\n",
    "    * 侧空翻、武术动作、跳舞——每个动作单独训练一个策略\n",
    "    * 通过 动作融合 或 分层 RL 切换技能\n",
    "\n",
    "* 跌倒保护与自主站起\n",
    "\n",
    "    * 检测即将跌倒 → 触发“受身”策略（类似柔道滚翻）\n",
    "    * 站起策略需协调全身 20+ 关节，RL 比手工编程更高效\n",
    "\n",
    "* 双臂-躯干协同\n",
    "\n",
    "    * 例如：单手支撑身体完成高抬腿（G1 展示过）\n",
    "    * 多任务 RL 同时优化平衡与操作目标\n",
    "\n",
    "⚠️ 挑战：\n",
    "\n",
    "人形机器人 sim-to-real gap 更大（质量分布、电机响应误差放大）\n",
    "安全风险高：真机训练成本昂贵，需谨慎部署\n",
    "宇树 G1 采用 “仿真预训练 + 真机微调” 策略降低风险\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c146d1f7",
   "metadata": {},
   "source": [
    "### 五、前沿趋势\n",
    "\n",
    "* 具身强化学习（Embodied RL）\n",
    "\n",
    "    * 将语言模型（如 LLM）与 RL 结合：\n",
    "\n",
    "    * “跳个舞” → LLM 解析为动作序列 → RL 执行，宇树 UnifoLM 正探索此方向\n",
    "\n",
    "* 离线强化学习（Offline RL）\n",
    "\n",
    "    * 利用历史数据（如遥操作日志）训练，减少真机交互\n",
    "\n",
    "* 多智能体 RL\n",
    "\n",
    "    * 多台 Go2 协同搬运、编队行进\n",
    "\n",
    "* 神经符号结合\n",
    "\n",
    "    * 用符号规则约束 RL 策略（如“禁止膝盖反向弯曲”），提升安全性\n",
    "\n",
    "### 六、局限与反思\n",
    "\n",
    "奖励函数设计仍是艺术：不好设计会导致“奖励黑客”（如机器人原地抖动刷分）\n",
    "\n",
    "样本效率低：需数百万次仿真交互，计算资源密集\n",
    "\n",
    "缺乏形式化安全保证：无法像 MPC 那样提供稳定性证明\n",
    "\n",
    "真机部署仍需工程调优：RL 输出常需滤波、限幅、与底层 PID 融合\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf478cee",
   "metadata": {},
   "source": [
    "\n",
    "### 总结\n",
    "强化学习正在重塑腿式机器人的运动智能：\n",
    "\n",
    "四足机器人：从“能走”到“走得稳、跑得快、玩得花”\n",
    "\n",
    "人形机器人：从“蹒跚学步”到“翻腾跳跃、人机共舞”\n",
    "\n",
    "宇树 Go2 和 G1 的惊艳表现，背后正是 大规模仿真 + PPO/SAC 算法 + 精巧奖励设计 + 域随机化 的工程结晶。未来，随着 世界模型（World Models） 和 通用机器人策略 的发展，RL 有望让机器人真正具备“在未知环境中自主适应”的能力。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
